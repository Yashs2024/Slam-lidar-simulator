<div align="center">

# ğŸ¤– SLAM & LiDAR Simulator

**An interactive, browser-based educational simulator for teaching SLAM, LiDAR raycasting, and autonomous robot navigation.**

[![Built with Vite](https://img.shields.io/badge/Built%20with-Vite-646CFF?style=for-the-badge&logo=vite)](https://vitejs.dev/)
[![Vanilla JS](https://img.shields.io/badge/Vanilla-JavaScript-F7DF1E?style=for-the-badge&logo=javascript)](https://developer.mozilla.org/en-US/docs/Web/JavaScript)
[![Chart.js](https://img.shields.io/badge/Chart.js-FF6384?style=for-the-badge&logo=chart.js&logoColor=white)](https://www.chartjs.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-green?style=for-the-badge)](./LICENSE)

</div>

---

## âœ¨ Live Demo

> Clone the repo and run `npm run dev` â€” it's instant with Vite.

---

## ğŸ—ºï¸ What Is This?

SLAM (**Simultaneous Localization and Mapping**) is one of the most important concepts in modern robotics. This project lets students *see* every step of the process in real time â€” no PhD required.

A differential-drive robot equipped with a 2D LiDAR sensor explores a 2D arena. As it drives, it builds an **occupancy grid map** of its environment from scratch â€” just using the noisy distance measurements from its rotating laser. You can set goals, watch it navigate autonomously with **A\* pathfinding**, draw your own obstacles, and see everything reflected in live charts and a fog-of-war map.

---

## ğŸš€ Features

| Feature | Description |
|---|---|
| ğŸ”´ **LiDAR Raycasting** | Up to 360 rays/frame using precise line-line intersection math |
| ğŸ—ºï¸ **Occupancy Grid SLAM** | Builds a "believed" map in real-time from noisy sensor data |
| ğŸŒ«ï¸ **Fog of War Vision** | SLAM map starts dark; explored areas are permanently revealed |
| ğŸ¤– **Collision Physics** | Robot cannot pass through walls (circle-line boundary detection) |
| ğŸ§­ **A\* Autonomous Navigation** | Click anywhere on the SLAM map to set a goal; robot drives itself |
| ğŸ—ï¸ **Interactive Build Mode** | Draw custom walls and obstacles directly on the canvas with your mouse |
| ğŸ“Š **Live Sensor Data Chart** | Real-time polar area chart of all 360 LiDAR measurements |
| ğŸ›ï¸ **Full UI Controls** | Sliders for noise, speed, and ray density; Drive/Build mode toggle |

---

## ğŸ“ Learning Objectives

This simulator is ideal for **3rd-year robotics and automation students** covering:

- **LiDAR / Range Sensor Fundamentals** â€” how 2D rotating laser rangefinders work
- **Sensor Noise Modelling** â€” observe how Gaussian error corrupts measurements
- **Occupancy Grid Mapping** â€” understand probabilistic free/occupied cell voting
- **SLAM Concepts** â€” see how a map emerges from sensor data and odometry
- **Path Planning** â€” visualise A\* searching the robot's own internal map
- **Robot Kinematics** â€” differential-drive with forward/angular velocity control

---

## ğŸ› ï¸ Tech Stack

- **[Vite](https://vitejs.dev/)** â€” Ultra-fast dev server and build tool
- **Vanilla JavaScript (ES Modules)** â€” Zero front-end framework overhead for maximum performance
- **HTML5 Canvas API** â€” Multi-layer composited rendering at 60fps
- **[Chart.js](https://www.chartjs.org/)** â€” Live polar-area sensor data chart
- **CSS Custom Properties** â€” Premium dark-mode design system

---

## ğŸ“¦ Getting Started

### Prerequisites

- **Node.js** â‰¥ 18
- **npm** â‰¥ 9

### Installation

```bash
# 1. Clone the repository
git clone https://github.com/YOUR_GITHUB_USERNAME/slam-lidar-simulator.git

# 2. Navigate into the project
cd slam-lidar-simulator

# 3. Install dependencies
npm install

# 4. Start the development server
npm run dev
```

Open your browser at **http://localhost:5173** and start exploring!

---

## ğŸ® How to Use

### Drive Mode (Default)

| Control | Action |
|---|---|
| `W` | Move Forward |
| `S` | Move Backward |
| `A` | Rotate Left |
| `D` | Rotate Right |
| Click on **SLAM Map** | Set autonomous navigation goal |

### Build Mode

1. Click **"Build Mode"** in the sidebar
2. **Click and drag** on the Real World canvas to draw walls
3. Switch back to **Drive Mode** to interact with the robot
4. Click **"Clear Custom Walls"** to erase your obstacles

### Viewport Toggle

- **Real World** â€” See the ground truth environment, walls, and LiDAR rays
- **SLAM Map** â€” See only what the robot *believes* based on its sensor data + fog of war

---

## ğŸ—ï¸ Project Architecture

```
slam-simulator/
â”œâ”€â”€ index.html              # App shell + Chart.js CDN
â”œâ”€â”€ style.css               # Dark-mode design system (CSS variables)
â””â”€â”€ src/
    â”œâ”€â”€ main.js             # Application bootstrap, game loop, event wiring
    â”œâ”€â”€ Renderer.js         # Canvas drawing: environment, robot, fog-of-war, paths
    â”œâ”€â”€ Environment.js      # Wall management (base + user-drawn custom walls)
    â”œâ”€â”€ Robot.js            # Differential-drive kinematics + collision physics
    â”œâ”€â”€ Lidar.js            # 2D raycasting + Gaussian noise + ray rendering
    â”œâ”€â”€ Mapper.js           # Occupancy grid (SLAM memory) + map rendering
    â”œâ”€â”€ AStar.js            # TypedArray-optimised A* pathfinding on the grid
    â””â”€â”€ ChartManager.js     # Chart.js wrapper for live LiDAR data visualisation
```

---

## ğŸ”¬ Algorithm Deep Dives

### LiDAR Raycasting
Each ray is cast from the robot's position at a given angle. The simulator checks it for intersection against every wall segment using standard computational geometry (parametric line-line intersection with `t` and `u` coefficients). The closest intersection becomes the sensor reading. Gaussian-like noise is then added.

### Occupancy Grid SLAM
The map is a 2D grid of cells. After each scan, cells along the ray path are marked **free** (known empty), and the cell at the hit point is marked **occupied** (wall detected). This is a simplified Bresenham-line voting scheme â€” the precursor to probabilistic methods like **Bayesian SLAM** and **Particle Filters**.

### A\* Pathfinding
The A\* implementation uses `Uint8Array` and `Float32Array` typed arrays for the open/closed sets, eliminating JavaScript string key allocations entirely. This keeps search times under **1ms** even on large maps.

### Fog of War (Canvas Compositing)
An off-screen `<canvas>` is filled entirely opaque (the "fog"). Each frame, the LiDAR hit polygon is drawn onto the fog canvas using the `destination-out` composite operation with a soft radial gradient. This **permanently erases** the fog wherever the sensor has swept, giving a smooth game-engine-style exploration effect.

---

## ğŸš§ Roadmap

- [ ] Particle Filter (Monte Carlo) SLAM
- [ ] Loop Closure detection + map correction
- [ ] 3D Three.js view (bird's eye + 3D perspective split)
- [ ] Exportable SLAM maps (PNG / JSON)
- [ ] Mobile touch support for Build Mode

---

## ğŸ“„ License

MIT License â€” see [LICENSE](./LICENSE) for details.

---

<div align="center">
Built with â¤ï¸ for robotics students everywhere.
</div>
